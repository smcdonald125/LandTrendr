{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Points for Accuracy Assessment\n",
    "Goal: Drop random points for 4 classes (Development, Tree Canopy, Vegetation, Wetlands) where change occurred and where no change occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import math\n",
    "\n",
    "# folder containing raster truth data\n",
    "folder = r\"C:\\Users\\smcdonald\\Documents\\Data\\GEE\\Wicomico\\AccuracyData\"\n",
    "lcs = ['dev', 'TC', 'veg', 'wet'] # 4 LCs of interest\n",
    "num_samples = 50 # number of samples per class and per change type (50 no change tc, 50 for change in TC)\n",
    "\n",
    "# path to LandTrendr results\n",
    "lt_folder = r\"C:\\Users\\smcdonald\\Documents\\Data\\GEE\\Wicomico\\LandTrendr\"\n",
    "\n",
    "# LandTrendr bands/indices \n",
    "lt_results =  ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'NDVI', 'NBR', 'NDMI', 'NDBI', 'TCB', 'TCG', 'TCW']\n",
    "bands = ['stYr', 'endYr', 'stVal', 'endVal', 'mag', 'dur', 'rate'] # bands in the LT results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(ary, values, num_samples):\n",
    "    \"\"\"\n",
    "    random_sample randomly sample num_samples of cells in the array that are equal to one of the values\n",
    "                    passed. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary : np.array\n",
    "        array to build points from\n",
    "    values : list\n",
    "        raster values to sample\n",
    "    num_samples : int\n",
    "        number of samples to select\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of tuples repesenting x,y indexes of the original array\n",
    "    \"\"\"\n",
    "    ns = num_samples\n",
    "     # mask values to sample from\n",
    "    sampling_matrix = np.where(np.isin(ary, values), 1, 0)\n",
    "\n",
    "     # subset array to nonzero values\n",
    "    n = np.nonzero(sampling_matrix)\n",
    "    sample_array = sampling_matrix[n]\n",
    "\n",
    "    # convert to probabilities\n",
    "    sample_array = sample_array / sample_array.sum()\n",
    "    l = len(sample_array)\n",
    "\n",
    "    # validate there are enough cells to sample\n",
    "    s_flag = True\n",
    "    vals, counts = np.unique(sampling_matrix, return_counts=True)\n",
    "    vals, counts = list(vals), list(counts)\n",
    "    if 1 not in vals:\n",
    "        print(f\"\\tNo cells to sample - skipping\")\n",
    "        s_flag = False\n",
    "    elif counts[vals.index(1)] < num_samples*4:\n",
    "        ns = math.floor((counts[vals.index(1)] / 4))\n",
    "        if ns >= 1:\n",
    "            print(f\"\\tNot enough cells to sample - reduced to {ns} from {counts[vals.index(1)]} cells\")\n",
    "        else:\n",
    "            print(f\"\\tNot enough cells to sample {counts[vals.index(1)]} - skipping\")\n",
    "            s_flag = False\n",
    "        \n",
    "    if s_flag:\n",
    "        # get row, col indices for random samples\n",
    "        s = np.random.choice(range(l), ns, p=sample_array, replace=False) \n",
    "        samples = [(n[0][i], n[1][i]) for i in s] # list of tuples\n",
    "\n",
    "        # return samples\n",
    "        return samples\n",
    "    else: # no samples, return empty list\n",
    "        return []\n",
    "\n",
    "def build_points(src, ary, samples, lc):\n",
    "    \"\"\"\n",
    "    build_points Convert the x,y raster indices to points. Constructs a geopandas geodataframe of the points \n",
    "                 and extracts the value of the array. Expects the array to have values 1,2,3, where 1 is no\n",
    "                 change, 2 is gain, and 3 is loss (for the specified lc). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src : rasterio object\n",
    "        open source object of the raster\n",
    "    ary : numpy.array\n",
    "        array used to create samples.\n",
    "    samples : list\n",
    "        list of raster x,y coordinates\n",
    "    lc : str\n",
    "        name of lc type that was sampled. Used for naming conventions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        Point database of the samples, with the value of the raster it was sampled from.\n",
    "    \"\"\"\n",
    "    # copy metadata\n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    # get crs\n",
    "    crs = meta['crs']\n",
    "    if crs.is_epsg_code:\n",
    "        epsg_code = int(crs['init'].lstrip('epsg:'))\n",
    "\n",
    "    # get upper left corner and cell size from metadata\n",
    "    top, left, x_cellSize, y_cellSize = meta['transform'][5], meta['transform'][2], meta['transform'][0], meta['transform'][4]\n",
    "\n",
    "    # iterate through the samples to construct their x,y points\n",
    "    points = [] # list to store points\n",
    "    values = [] # list of the raster value, same index as points\n",
    "    for sample in samples:\n",
    "        # construct x and y values\n",
    "        x = sample[1] * x_cellSize + left + (x_cellSize / 2) # add half of cell size to center point\n",
    "        y = sample[0] * y_cellSize + top + (y_cellSize / 2) # add half of cell size to center point\n",
    "\n",
    "        # convert to point geometry\n",
    "        pt = Point((x,y))\n",
    "\n",
    "        # add point list of points\n",
    "        points.append(pt)\n",
    "\n",
    "        # extract raster value\n",
    "        val = ary[sample[0]][sample[1]]\n",
    "        if val not in [1,2,3]:\n",
    "            print(f\"ERROR SAMPLE: {val}. {sample}\")\n",
    "        values.append(val)\n",
    "\n",
    "    # convert to geodataframe\n",
    "    point_gdf = gpd.GeoDataFrame(geometry=points, data={'value':values}, crs=f\"EPSG:{epsg_code}\")\n",
    "\n",
    "    # define string name\n",
    "    point_gdf.loc[point_gdf['value'] == 1, 'lc'] = lc\n",
    "    point_gdf.loc[point_gdf['value'] == 2, 'lc'] = f\"{lc}_gain\"\n",
    "    point_gdf.loc[point_gdf['value'] == 3, 'lc'] = f\"{lc}_loss\"\n",
    "\n",
    "    # return points\n",
    "    return point_gdf\n",
    "\n",
    "def extract_values_at_points(boi, gdf):\n",
    "    \"\"\"\n",
    "    extract_values_at_points For the raster of interest, extract the raster value for each point\n",
    "                             in the geodataframe and add them as new columns in the geodataframe.\n",
    "                             Expects the raster to contain the number and name of bands listed\n",
    "                             in the first section. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    boi : str\n",
    "        Name of band of interest. Used to construct raster path to sample.\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        Point database.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        Point database with added columns of sampled raster values.\n",
    "    \"\"\"\n",
    "    # for the band of interest (boi), extract the LT result values for the points\n",
    "    ras_path = f\"{lt_folder}/LT_Wico_{boi}.tif\"\n",
    "\n",
    "    # create list of coords\n",
    "    coord_list = [(x, y) for x, y in zip(gdf[\"geometry\"].x, gdf[\"geometry\"].y)]\n",
    "\n",
    "    # build column names\n",
    "    if 'SR' in boi:\n",
    "        pf = f\"{boi.split('_')[-1]}\"\n",
    "    else:\n",
    "        pf = boi\n",
    "    new_cols = [f\"{pf}_{x}\" for x in bands]\n",
    "\n",
    "    # open the raster\n",
    "    with rio.open(ras_path) as src:\n",
    "        gdf[new_cols] = [x for x in src.sample(coord_list)]\n",
    "    \n",
    "    # return the points with data extracted\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate the Rasters and Randomly Sample Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of geodataframes\n",
    "gdf_list = []\n",
    "\n",
    "# iterate over the land covers\n",
    "for lc in lcs:\n",
    "    print(f\"Starting {lc}...\")\n",
    "    ras_path = f\"{folder}/Wico_AA_{lc}_30m.tif\"\n",
    "    # open the raster\n",
    "    with rio.open(ras_path) as src:\n",
    "        # read in the array\n",
    "        ary = src.read(1)\n",
    "\n",
    "        # pull samples for change\n",
    "        chg_samples = random_sample(ary, [2, 3], num_samples)\n",
    "\n",
    "        # pull samples for no change\n",
    "        st_samples = random_sample(ary, [1], num_samples)\n",
    "\n",
    "        # merge samples into single list of indices\n",
    "        samples = chg_samples + st_samples\n",
    "\n",
    "        # create points and extract cell values\n",
    "        tmp_gdf = build_points(src, ary, samples, lc)\n",
    "\n",
    "        # add gdf to list\n",
    "        gdf_list.append(tmp_gdf.copy())\n",
    "        del tmp_gdf\n",
    "\n",
    "# concat gdfs into single gdf\n",
    "sample_points = pd.concat(gdf_list).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# extract LT results for the points\n",
    "for b in lt_results:\n",
    "    sample_points = extract_values_at_points(b, sample_points)\n",
    "\n",
    "# write points\n",
    "sample_points.to_file(f\"{folder}/AA_points.shp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify max values per spectral index to normalize between 0 and 1 (or -1 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize data - define max values per band/index\n",
    "# TODO this should be moved up when reading these in the first time\n",
    "normalize_ = {}\n",
    "for b in lt_results:\n",
    "    # normalized difference indices are aleady 0-1\n",
    "    if b in ['NDVI', 'NBR', 'NDMI', 'NDBI']:\n",
    "        continue\n",
    "\n",
    "    # read in ary\n",
    "    ras_path = f\"{lt_folder}/LT_Wico_{b}.tif\"\n",
    "    with rio.open(ras_path) as src:\n",
    "        mx = 0.0\n",
    "        for i in [3, 4]: # start and end values\n",
    "            # read array\n",
    "            ary = src.read(7)\n",
    "\n",
    "            # extract max value\n",
    "            m1 = abs(np.nanmax(ary))\n",
    "            m2 = abs(np.nanmin(ary))\n",
    "            ary = None\n",
    "\n",
    "            # correct for negatives\n",
    "            if m2 > m1:\n",
    "                m1 = m2\n",
    "            \n",
    "            # update max\n",
    "            if m1 > mx:\n",
    "                mx = m1\n",
    "\n",
    "        # set max value in the dict\n",
    "        normalize_[b] = mx\n",
    "\n",
    "        print(b, mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize sample point change magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_points is None:\n",
    "    sample_points = gpd.read_file(f\"{folder}/AA_points.shp\")\n",
    "\n",
    "# normalize magnitudes\n",
    "for n in normalize_:\n",
    "    if 'SR_' in n:\n",
    "        c = f\"{n.split('_')[-1]}_mag\"\n",
    "    else:\n",
    "        c = f\"{n}_mag\"\n",
    "\n",
    "    sample_points.loc[:, f\"{c}_n\"] = sample_points[c] / normalize_[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Producer's and User's Accuracies\n",
    "Calculate PA and UA for change versus no change for each class.\n",
    "Construct matrix for each band/index and lc combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# table to store results\n",
    "df = pd.DataFrame(columns=['landcover', 'index', 'nc_nc', 'c_c', 'nc_c', 'c_nc', 'com_nc', 'com_c', 'om_nc', 'om_c'])\n",
    "\n",
    "for lc in lcs:\n",
    "    for b in lt_results:\n",
    "        if 'SR' in b:\n",
    "            boi = f\"{b.split('_')[-1]}_mag_n\"\n",
    "        elif b in ['NDVI', 'NDMI', 'NDBI', 'NBR']:\n",
    "            boi = f\"{b}_mag\"\n",
    "        else:\n",
    "            boi = f\"{b}_mag_n\"\n",
    "\n",
    "        # calculate number of agreement samples\n",
    "        nc_nc = len(sample_points.query(f\"lc == '{lc}' and ({boi} < 0.05 and {boi} > -0.05)\"))\n",
    "        c_c = len(sample_points.query(f\"(lc == '{lc}_gain' or lc == '{lc}_loss') and ({boi} >= 0.05 or {boi} <= -0.05)\"))\n",
    "\n",
    "        # calculate disagreement samples\n",
    "        c_nc = len(sample_points.query(f\"lc == '{lc}' and ({boi} >= 0.05 or {boi} <= -0.05)\"))\n",
    "        nc_c = len(sample_points.query(f\"(lc == '{lc}_gain' or lc == '{lc}_loss') and ({boi} < 0.05 and {boi} > -0.05)\"))\n",
    "\n",
    "        # commission errors\n",
    "        com_nc = round( (nc_nc / (nc_nc + nc_c)), 4)\n",
    "        com_c = round( (c_c / (c_c + c_nc) ), 4)\n",
    "\n",
    "        # omission errors\n",
    "        om_nc = round( (nc_nc / (nc_nc + c_nc) ), 4)\n",
    "        om_c = round( (c_c / (c_c + nc_c) ), 4)\n",
    "\n",
    "        # add data to table\n",
    "        df.loc[len(df)] = [lc, boi.split('_')[0], nc_nc, c_c, nc_c, c_nc, com_nc, com_c, om_nc, om_c]\n",
    "\n",
    "        # create matrix\n",
    "        \"\"\"\n",
    "        | col truth, rows LT | No change            | Change                   |\n",
    "        | No Change          | lc where band < 0.05 | lc_chg where band < 0.05 |\n",
    "        | Change             | lc where band > 0.05 | lc where band > 0.05     |\n",
    "        \"\"\"\n",
    "        matrix = pd.DataFrame()\n",
    "\n",
    "        # add counts\n",
    "        matrix.loc[:, boi.split('_')[0]] = [\"No Change\", \"Change\"] # add row /result names\n",
    "        matrix.loc[:, \"No Change\"] = [nc_nc, c_nc] # add values in columns\n",
    "        matrix.loc[:, \"Change\"] = [nc_c, c_c] # add values in columns\n",
    "\n",
    "        # add totals\n",
    "        matrix.loc[:, 'Total'] = [nc_nc+nc_c, c_nc+c_c]\n",
    "        matrix.loc[len(matrix)] = ['Total', nc_nc+c_nc, nc_c+c_c, nc_nc+nc_c+c_nc+c_c]\n",
    "\n",
    "        # add producer and user accuracies\n",
    "        matrix.loc[:, \"User's Accuracy\"] = [com_nc, com_c, np.nan]\n",
    "        matrix.loc[len(matrix)] = [\"Producer's Accuracy\", om_nc, om_c, np.nan, np.nan]\n",
    "\n",
    "        # write matrix\n",
    "        matrix.to_csv(f\"{folder}/tables/matrices/{lc}_{boi.split('_')[0]}_matrix.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
